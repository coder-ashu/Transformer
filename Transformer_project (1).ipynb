{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLpJZbd0hSxh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7G49yDHZg-WI",
        "outputId": "ee1ad5d2-fa6c-413c-bd79-cf918a5b1e92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ov0LMgIFK8LK"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "9a2yX27TLh1v",
        "outputId": "d7cd8dc9-6b45-4d01-f71d-a14dab0e7a3b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8781d454-9a5f-4a3f-a8b0-d04460707695\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8781d454-9a5f-4a3f-a8b0-d04460707695\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"ashutosh0410\",\"key\":\"d4bfc0ea3ae09bb09e1b3f9a9950e9d1\"}'}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()  # Select your kaggle.json file here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67vesSh8LmJd"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-yCvSaiLotk",
        "outputId": "eebe8fe7-c364-4ad5-aaa0-980bcb8870d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/thedevastator/lambada-word-prediction-dataset\n",
            "License(s): CC0-1.0\n",
            "Downloading lambada-word-prediction-dataset.zip to /content/dataset\n",
            " 97% 316M/326M [00:05<00:00, 37.2MB/s]\n",
            "100% 326M/326M [00:05<00:00, 63.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d thedevastator/lambada-word-prediction-dataset -p /content/dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lwn0xmqMLsJw",
        "outputId": "c1897deb-76ce-4f3f-f46c-fc739ed33c16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/dataset/lambada-word-prediction-dataset.zip\n",
            "replace /content/dataset/test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!unzip /content/dataset/lambada-word-prediction-dataset.zip -d /content/dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "vr8NkusvOiqb",
        "outputId": "7431fa8a-6286-4f63-aa73-dcc552ad28fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lambada-word-prediction-dataset.zip  test.csv  train.csv  validation.csv\n"
          ]
        }
      ],
      "source": [
        "!ls /content/dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb2q_8VSPU5Y",
        "outputId": "8a58a6fb-8285-4f39-a720-c827a0bdcb89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-11-09 11:13:24--  https://www.gutenberg.org/files/100/100-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5618733 (5.4M) [text/plain]\n",
            "Saving to: ‘/content/shakespeare.txt’\n",
            "\n",
            "/content/shakespear 100%[===================>]   5.36M  25.8MB/s    in 0.2s    \n",
            "\n",
            "2024-11-09 11:13:24 (25.8 MB/s) - ‘/content/shakespeare.txt’ saved [5618733/5618733]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.gutenberg.org/files/100/100-0.txt -O /content/shakespeare.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FVkoZFiQSGXd"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/shakespeare.txt\", \"r\") as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Rj1jbYaSqOA",
        "outputId": "2f1d1b08-f4d0-43cd-846f-6b0ece5612c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lenght of the dataset is  5359439\n"
          ]
        }
      ],
      "source": [
        "print(\"lenght of the dataset is \", len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "8I2LkI9wSyY1",
        "outputId": "f6b6b82a-d7fd-44b1-9a62-bb51c4382f95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\n",
            " !&'()*,-.0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyzÀÆÇÉàâæçèéêëîœ—‘’“”…\n",
            "100\n"
          ]
        }
      ],
      "source": [
        "chars=sorted(set(text))\n",
        "vocab_size=len(chars)\n",
        "print(\"\".join(chars))\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvSiYV49VKbQ",
        "outputId": "273bc7ab-1c9c-4974-ac74-6a1ca935cfab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['*', '*', '*', ' ', 'S', 'T', 'A', 'R', 'T', ' ', 'O', 'F', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'J', 'E', 'C', 'T', ' ', 'G', 'U', 'T', 'E', 'N', 'B', 'E', 'R', 'G', ' ', 'E', 'B', 'O', 'O', 'K', ' ', '', '', '', ' ', '*', '*', '*', '\\n', 'T', 'h', 'e', ' ', 'C', 'o', 'm', 'p', 'l', 'e', 't', 'e', ' ', 'W', 'o', 'r', 'k', 's', ' ', 'o', 'f', ' ', 'W', 'i', 'l', 'l', 'i', 'a', 'm', ' ', 'S', 'h', 'a', 'k', 'e', 's', 'p', 'e', 'a', 'r', 'e', '\\n', '\\n', 'b', 'y', ' ', 'W', 'i', 'l', 'l', 'i', 'a', 'm', ' ', 'S', 'h', 'a', 'k', 'e', 's', 'p', 'e', 'a', 'r', 'e', '\\n', '\\n', '\\n', '\\n', '\\n', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'C', 'o', 'n', 't', 'e', 'n', 't', 's', '\\n', '\\n', ' ', ' ', ' ', ' ', 'T', 'H', 'E', ' ', 'S', 'O', 'N', 'N', 'E', 'T', 'S', '\\n', ' ', ' ', ' ', ' ', 'A', 'L', 'L', '’', 'S', ' ', 'W', 'E', 'L', 'L', ' ', 'T', 'H', 'A', 'T', ' ', 'E', 'N', 'D', 'S', ' ', 'W', 'E', 'L', 'L', '\\n', ' ', ' ', ' ', ' ', 'T', 'H', 'E', ' ', 'T', 'R', 'A', 'G', 'E', 'D', 'Y', ' ', 'O', 'F', ' ', 'A', 'N', 'T', 'O', 'N', 'Y', ' ', 'A', 'N', 'D', ' ', 'C', 'L', 'E', 'O', 'P', 'A', 'T', 'R', 'A', '\\n', ' ', ' ', ' ', ' ', 'A', 'S', ' ', 'Y', 'O', 'U', ' ', 'L', 'I', 'K', 'E', ' ', 'I', 'T', '\\n', ' ', ' ', ' ', ' ', 'T', 'H', 'E', ' ', 'C', 'O', 'M', 'E', 'D', 'Y', ' ', 'O', 'F', ' ', 'E', 'R', 'R', 'O', 'R', 'S', '\\n', ' ', ' ', ' ', ' ', 'T', 'H', 'E', ' ', 'T', 'R', 'A', 'G', 'E', 'D', 'Y', ' ', 'O', 'F', ' ', 'C', 'O', 'R', 'I', 'O', 'L', 'A', 'N', 'U', 'S', '\\n', ' ', ' ', ' ', ' ', 'C', 'Y', 'M', 'B', 'E', 'L', 'I', 'N', 'E', '\\n', ' ', ' ', ' ', ' ', 'T', 'H', 'E', ' ', 'T', 'R', 'A', 'G', 'E', 'D', 'Y', ' ', 'O', 'F', ' ', 'H', 'A', 'M', 'L', 'E', 'T', ',', ' ', 'P', 'R', 'I', 'N', 'C', 'E', ' ', 'O', 'F', ' ', 'D', 'E', 'N', 'M', 'A', 'R', 'K', '\\n', ' ', ' ', ' ', ' ', 'T', 'H', 'E', ' ', 'F', 'I', 'R', 'S', 'T', ' ', 'P', 'A', 'R', 'T', ' ', 'O', 'F', ' ', 'K', 'I', 'N', 'G', ' ', 'H', 'E', 'N', 'R', 'Y', ' ', 'T', 'H', 'E', ' ', 'F', 'O', 'U', 'R', 'T', 'H', '\\n', ' ', ' ', ' ', ' ', 'T', 'H', 'E', ' ', 'S', 'E', 'C', 'O', 'N', 'D', ' ', 'P', 'A', 'R', 'T', ' ', 'O', 'F', ' ', 'K', 'I', 'N', 'G', ' ', 'H', 'E', 'N', 'R', 'Y', ' ', 'T', 'H', 'E', ' ', 'F', 'O', 'U', 'R', 'T', 'H', '\\n', ' ', ' ', ' ', ' ', 'T', 'H', 'E', ' ', 'L', 'I', 'F', 'E', ' ', 'O', 'F', ' ', 'K', 'I', 'N', 'G', ' ', 'H', 'E', 'N', 'R', 'Y', ' ', 'T', 'H', 'E', ' ', 'F', 'I', 'F', 'T', 'H', '\\n', ' ', ' ', ' ', ' ', 'T', 'H', 'E', ' ', 'F', 'I', 'R', 'S', 'T', ' ', 'P', 'A', 'R', 'T', ' ', 'O', 'F', ' ', 'H', 'E', 'N', 'R', 'Y', ' ', 'T', 'H', 'E', ' ', 'S', 'I', 'X', 'T', 'H', '\\n', ' ', ' ', ' ', ' ', 'T', 'H', 'E', ' ', 'S', 'E', 'C', 'O', 'N', 'D', ' ', 'P', 'A', 'R', 'T', ' ', 'O', 'F', ' ', 'K', 'I', 'N', 'G', ' ', 'H', 'E', 'N', 'R', 'Y', ' ', 'T', 'H', 'E', ' ', 'S', 'I', 'X', 'T', 'H', '\\n', ' ', ' ', ' ', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'I', 'R', 'D', ' ', 'P', 'A', 'R', 'T', ' ', 'O', 'F', ' ', 'K', 'I', 'N', 'G', ' ', 'H', 'E', 'N', 'R', 'Y', ' ', 'T', 'H', 'E', ' ', 'S', 'I', 'X', 'T', 'H', '\\n', ' ', ' ', ' ', ' ', 'K', 'I', 'N', 'G', ' ', 'H', 'E', 'N', 'R', 'Y', ' ', 'T', 'H', 'E', ' ', 'E', 'I', 'G', 'H', 'T', 'H', '\\n', ' ', ' ', ' ', ' ', 'T', 'H', 'E', ' ', 'L', 'I', 'F', 'E', ' ', 'A', 'N', 'D', ' ', 'D', 'E', 'A', 'T', 'H', ' ', 'O', 'F', ' ', 'K', 'I', 'N', 'G', ' ', 'J', 'O', 'H', 'N', '\\n', ' ', ' ', ' ', ' ', 'T', 'H', 'E', ' ', 'T', 'R', 'A', 'G', 'E', 'D', 'Y', ' ', 'O', 'F', ' ', 'J', 'U', 'L', 'I', 'U', 'S', ' ', 'C', 'A', 'E', 'S', 'A', 'R', '\\n', ' ', ' ', ' ', ' ', 'T', 'H', 'E', ' ', 'T', 'R', 'A', 'G', 'E', 'D', 'Y', ' ', 'O', 'F', ' ', 'K', 'I', 'N', 'G', ' ', 'L', 'E', 'A', 'R', '\\n', ' ', ' ', ' ', ' ', 'L', 'O', 'V', 'E', '’', 'S', ' ', 'L', 'A', 'B', 'O', 'U', 'R', '’', 'S', ' ', 'L', 'O', 'S', 'T', '\\n', ' ', ' ', ' ', ' ', 'T', 'H', 'E', ' ', 'T', 'R', 'A', 'G', 'E', 'D', 'Y', ' ', 'O', 'F', ' ', 'M', 'A', 'C', 'B', 'E', 'T', 'H', '\\n', ' ', ' ', ' ', ' ', 'M', 'E', 'A', 'S', 'U', 'R', 'E', ' ', 'F', 'O', 'R', ' ', 'M', 'E', 'A', 'S', 'U', 'R', 'E', '\\n', ' ', ' ', ' ', ' ', 'T', 'H', 'E', ' ', 'M', 'E', 'R', 'C', 'H', 'A', 'N', 'T', ' ', 'O', 'F', ' ', 'V', 'E', 'N', 'I', 'C', 'E', '\\n', ' ', ' ', ' ', ' ', 'T', 'H', 'E', ' ', 'M', 'E', 'R', 'R', 'Y', ' ', 'W', 'I', 'V', 'E', 'S', ' ', 'O', 'F', ' ', 'W', 'I', 'N', 'D', 'S', 'O', 'R', '\\n', ' ', ' ', ' ', ' ', 'A', ' ', 'M', 'I', 'D', 'S', 'U', 'M', 'M', 'E', 'R', ' ', 'N', 'I', 'G', 'H', 'T', '’', 'S', ' ', 'D', 'R', 'E', 'A', 'M', '\\n', ' ', ' ', ' ', ' ', 'M', 'U', 'C', 'H', ' ', 'A', 'D', 'O', ' ', 'A', 'B', 'O', 'U', 'T', ' ', 'N', 'O', 'T', 'H', 'I', 'N', 'G', '\\n', ' ', ' ', ' ', ' ', 'T', 'H', 'E', ' ', 'T', 'R', 'A', 'G', 'E', 'D', 'Y', ' ', 'O', 'F', ' ', 'O', 'T', 'H', 'E', 'L', 'L', 'O', ',', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'O', 'R', ' ', 'O', 'F', ' ', 'V', 'E', 'N', 'I', 'C', 'E', '\\n', ' ', ' ', ' ', ' ', 'P', 'E', 'R', 'I', 'C', 'L', 'E', 'S', ',', ' ', 'P', 'R', 'I', 'N', 'C']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import re\n",
        "\n",
        "# Remove numbers\n",
        "cleaned_data = [re.sub(r'\\d+', '', i) for i in text]\n",
        "\n",
        "# Display the result\n",
        "print(cleaned_data[:1000])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYYudHsMWXzB",
        "outputId": "d2823cb1-fff3-413a-93bf-556890b2a2e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\n",
            " !&'()*,-.:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyzÀÆÇÉàâæçèéêëîœ—‘’“”…\n",
            "91\n"
          ]
        }
      ],
      "source": [
        "chars=sorted(set(cleaned_data))\n",
        "vocab_size=len(chars)\n",
        "print(\"\".join(chars))\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4QLTW23dVpZF"
      },
      "outputs": [],
      "source": [
        "# creating an encoding for the cleaned data\n",
        "stoi={ch:i for i,ch in enumerate(chars)}\n",
        "itos={i:ch for i,ch in enumerate(chars)}\n",
        "encode=lambda s:[stoi[c] for c in s]\n",
        "decode=lambda l:\"\".join([itos[i] for i in l])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "OROULtonXYFF",
        "outputId": "d256f32c-40dd-4fa1-9e67-f9cd3a7106bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[52, 49, 56, 56, 59, 3, 67, 59, 62, 56, 48, 3]\n"
          ]
        }
      ],
      "source": [
        "print(encode(\"hello world \"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "O6-tHtKTXnwG"
      },
      "outputs": [],
      "source": [
        "# converting the data to tensor\n",
        "torch_data=torch.tensor(encode(cleaned_data),dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mdN9yhJFaKGw",
        "outputId": "7d5da7d9-6691-4eba-e655-33d425f50009"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 9,  9,  9,  3, 34, 35, 16, 33, 35,  3, 30, 21,  3, 35, 23, 20,  3, 31,\n",
            "        33, 30, 25, 20, 18, 35,  3, 22, 36, 35, 20, 29, 17, 20, 33, 22,  3, 20,\n",
            "        17, 30, 30, 26,  3,  0,  0,  0,  3,  9,  9,  9,  2, 35, 52, 49,  3, 18,\n",
            "        59, 57, 60, 56, 49, 64, 49,  3, 38, 59, 62, 55, 63,  3, 59, 50,  3, 38,\n",
            "        53, 56, 56, 53, 45, 57,  3, 34, 52, 45, 55, 49, 63, 60, 49, 45, 62, 49,\n",
            "         2,  2, 46, 69,  3, 38, 53, 56, 56, 53, 45, 57,  3, 34, 52, 45, 55, 49,\n",
            "        63, 60, 49, 45, 62, 49,  2,  2,  2,  2,  2,  3,  3,  3,  3,  3,  3,  3,\n",
            "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3, 18, 59, 58, 64, 49,\n",
            "        58, 64, 63,  2,  2,  3,  3,  3,  3, 35, 23, 20,  3, 34, 30, 29, 29, 20,\n",
            "        35, 34,  2,  3,  3,  3,  3, 16, 27, 27, 87, 34,  3, 38, 20, 27, 27,  3,\n",
            "        35, 23, 16, 35,  3, 20, 29, 19, 34,  3, 38, 20, 27, 27,  2,  3,  3,  3,\n",
            "         3, 35, 23, 20,  3, 35, 33, 16, 22, 20, 19, 40,  3, 30, 21,  3, 16, 29,\n",
            "        35, 30, 29, 40,  3, 16, 29, 19,  3, 18, 27, 20, 30, 31, 16, 35, 33, 16,\n",
            "         2,  3,  3,  3,  3, 16, 34,  3, 40, 30, 36,  3, 27, 24, 26, 20,  3, 24,\n",
            "        35,  2,  3,  3,  3,  3, 35, 23, 20,  3, 18, 30, 28, 20, 19, 40,  3, 30,\n",
            "        21,  3, 20, 33, 33, 30, 33, 34,  2,  3,  3,  3,  3, 35, 23, 20,  3, 35,\n",
            "        33, 16, 22, 20, 19, 40,  3, 30, 21,  3, 18, 30, 33, 24, 30, 27, 16, 29,\n",
            "        36, 34,  2,  3,  3,  3,  3, 18, 40, 28, 17, 20, 27, 24, 29, 20,  2,  3,\n",
            "         3,  3,  3, 35, 23, 20,  3, 35, 33, 16, 22, 20, 19, 40,  3, 30, 21,  3,\n",
            "        23, 16, 28, 27, 20, 35, 10,  3, 31, 33, 24, 29, 18, 20,  3, 30, 21,  3,\n",
            "        19, 20, 29, 28, 16, 33, 26,  2,  3,  3,  3,  3, 35, 23, 20,  3, 21, 24,\n",
            "        33, 34, 35,  3, 31, 16, 33, 35,  3, 30, 21,  3, 26, 24, 29, 22,  3, 23,\n",
            "        20, 29, 33, 40,  3, 35, 23, 20,  3, 21, 30, 36, 33, 35, 23,  2,  3,  3,\n",
            "         3,  3, 35, 23, 20,  3, 34, 20, 18, 30, 29, 19,  3, 31, 16, 33, 35,  3,\n",
            "        30, 21,  3, 26, 24, 29, 22,  3, 23, 20, 29, 33, 40,  3, 35, 23, 20,  3,\n",
            "        21, 30, 36, 33, 35, 23,  2,  3,  3,  3,  3, 35, 23, 20,  3, 27, 24, 21,\n",
            "        20,  3, 30, 21,  3, 26, 24, 29, 22,  3, 23, 20, 29, 33, 40,  3, 35, 23,\n",
            "        20,  3, 21, 24, 21, 35, 23,  2,  3,  3,  3,  3, 35, 23, 20,  3, 21, 24,\n",
            "        33, 34, 35,  3, 31, 16, 33, 35,  3, 30, 21,  3, 23, 20, 29, 33, 40,  3,\n",
            "        35, 23, 20,  3, 34, 24, 39, 35, 23,  2,  3,  3,  3,  3, 35, 23, 20,  3,\n",
            "        34, 20, 18, 30, 29, 19,  3, 31, 16, 33, 35,  3, 30, 21,  3, 26, 24, 29,\n",
            "        22,  3, 23, 20, 29, 33, 40,  3, 35, 23, 20,  3, 34, 24, 39, 35, 23,  2,\n",
            "         3,  3,  3,  3, 35, 23, 20,  3, 35, 23, 24, 33, 19,  3, 31, 16, 33, 35,\n",
            "         3, 30, 21,  3, 26, 24, 29, 22,  3, 23, 20, 29, 33, 40,  3, 35, 23, 20,\n",
            "         3, 34, 24, 39, 35, 23,  2,  3,  3,  3,  3, 26, 24, 29, 22,  3, 23, 20,\n",
            "        29, 33, 40,  3, 35, 23, 20,  3, 20, 24, 22, 23, 35, 23,  2,  3,  3,  3,\n",
            "         3, 35, 23, 20,  3, 27, 24, 21, 20,  3, 16, 29, 19,  3, 19, 20, 16, 35,\n",
            "        23,  3, 30, 21,  3, 26, 24, 29, 22,  3, 25, 30, 23, 29,  2,  3,  3,  3,\n",
            "         3, 35, 23, 20,  3, 35, 33, 16, 22, 20, 19, 40,  3, 30, 21,  3, 25, 36,\n",
            "        27, 24, 36, 34,  3, 18, 16, 20, 34, 16, 33,  2,  3,  3,  3,  3, 35, 23,\n",
            "        20,  3, 35, 33, 16, 22, 20, 19, 40,  3, 30, 21,  3, 26, 24, 29, 22,  3,\n",
            "        27, 20, 16, 33,  2,  3,  3,  3,  3, 27, 30, 37, 20, 87, 34,  3, 27, 16,\n",
            "        17, 30, 36, 33, 87, 34,  3, 27, 30, 34, 35,  2,  3,  3,  3,  3, 35, 23,\n",
            "        20,  3, 35, 33, 16, 22, 20, 19, 40,  3, 30, 21,  3, 28, 16, 18, 17, 20,\n",
            "        35, 23,  2,  3,  3,  3,  3, 28, 20, 16, 34, 36, 33, 20,  3, 21, 30, 33,\n",
            "         3, 28, 20, 16, 34, 36, 33, 20,  2,  3,  3,  3,  3, 35, 23, 20,  3, 28,\n",
            "        20, 33, 18, 23, 16, 29, 35,  3, 30, 21,  3, 37, 20, 29, 24, 18, 20,  2,\n",
            "         3,  3,  3,  3, 35, 23, 20,  3, 28, 20, 33, 33, 40,  3, 38, 24, 37, 20,\n",
            "        34,  3, 30, 21,  3, 38, 24, 29, 19, 34, 30, 33,  2,  3,  3,  3,  3, 16,\n",
            "         3, 28, 24, 19, 34, 36, 28, 28, 20, 33,  3, 29, 24, 22, 23, 35, 87, 34,\n",
            "         3, 19, 33, 20, 16, 28,  2,  3,  3,  3,  3, 28, 36, 18, 23,  3, 16, 19,\n",
            "        30,  3, 16, 17, 30, 36, 35,  3, 29, 30, 35, 23, 24, 29, 22,  2,  3,  3,\n",
            "         3,  3, 35, 23, 20,  3, 35, 33, 16, 22, 20, 19, 40,  3, 30, 21,  3, 30,\n",
            "        35, 23, 20, 27, 27, 30, 10,  3, 35, 23, 20,  3, 28, 30, 30, 33,  3, 30,\n",
            "        21,  3, 37, 20, 29, 24, 18, 20,  2,  3,  3,  3,  3, 31, 20, 33, 24, 18,\n",
            "        27, 20, 34, 10,  3, 31, 33, 24, 29, 18])\n"
          ]
        }
      ],
      "source": [
        "print(torch_data[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IAJ2wY9vaRvn"
      },
      "outputs": [],
      "source": [
        "train_data=torch_data[:int(0.9*len(torch_data))]\n",
        "val_data=torch_data[int(0.9*len(torch_data)):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kHLeIQeuaepk"
      },
      "outputs": [],
      "source": [
        "# setting up block size and batch size\n",
        "block_size=64\n",
        "batch_size=126\n",
        "n_embd=384\n",
        "dropout=0.2\n",
        "n_head=6\n",
        "n_layer=6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yef9UqFqA9k",
        "outputId": "1e190204-677d-4878-a80a-c5d3ba58865c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[49, 49, 48,  ..., 52, 64, 10],\n",
            "        [62, 53, 45,  ..., 62,  3, 52],\n",
            "        [35, 52, 49,  ..., 16, 58, 48],\n",
            "        ...,\n",
            "        [47, 49, 10,  ..., 43,  2,  2],\n",
            "        [65, 47, 55,  ...,  3, 45, 58],\n",
            "        [22, 30, 29,  ..., 65, 63,  3]])\n",
            "tensor([[49, 48, 63,  ..., 64, 10,  3],\n",
            "        [53, 45, 58,  ...,  3, 52, 49],\n",
            "        [52, 49, 53,  ..., 58, 48,  3],\n",
            "        ...,\n",
            "        [49, 10,  2,  ...,  2,  2, 27],\n",
            "        [47, 55, 56,  ..., 45, 58, 48],\n",
            "        [30, 29, 20,  ..., 63,  3, 59]])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "def get_batch(val):\n",
        "    data=train_data if val==\"train\" else val_data\n",
        "    ix=torch.randint(len(data)-block_size,(batch_size,))\n",
        "    x=torch.stack([data[i: i+block_size] for i in ix])\n",
        "    y=torch.stack([data[i+1: i+block_size+1] for i in ix])\n",
        "    return x,y\n",
        "\n",
        "xb,yb=get_batch(\"train\")\n",
        "print(xb)\n",
        "print(yb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "moCv1PLRu_kT"
      },
      "outputs": [],
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "    def __init__(self,num_heads,head_size):\n",
        "        super().__init__()\n",
        "        self.proj=nn.Linear(n_embd,n_embd)\n",
        "        self.heads=nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self,x):\n",
        "         out=torch.cat([h(x) for h in self.heads],dim=-1)\n",
        "         out=self.dropout(self.proj(out))\n",
        "         return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BhHBva-Dx1GB"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self,n_embd):\n",
        "        super().__init__()\n",
        "        self.net=nn.Sequential(\n",
        "            nn.Linear(n_embd,4*n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4*n_embd,n_embd),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "      return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rhJC40GK4Gp4"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self,n_embd,n_head):\n",
        "      # n_embd: embedding dimension , n_head: the number of heads we would like\n",
        "        super().__init__()\n",
        "        head_size=n_embd//n_head\n",
        "        self.sa=MultiheadAttention(n_head,head_size)\n",
        "        self.ffd=FeedForward(n_embd)\n",
        "        self.ln1=nn.LayerNorm(n_embd)\n",
        "        self.ln2=nn.LayerNorm(n_embd)\n",
        "    def forward(self,x):\n",
        "      x=x+self.sa(self.ln1(x))\n",
        "      x=x+self.ffd(self.ln2(x))\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_iDd_4U38z5J"
      },
      "outputs": [],
      "source": [
        "class BatchNorm1d(nn.Module):\n",
        "    def __init__(self,dim,eps=1e-5,momentum=0.1):\n",
        "        super().__init__()\n",
        "        self.eps=eps\n",
        "        self.gamma=nn.Parameter(torch.ones(dim))\n",
        "        self.beta=nn.Parameter(torch.zeros(dim))\n",
        "    def __call__self(self,x):\n",
        "        xmean=x.mean(1,keepdim=True)\n",
        "        xvar=(x-xmean).pow(2).mean(1,keepdim=True)\n",
        "        xhat=(x-xmean)/torch.sqrt(xvar+self.eps)\n",
        "        self.out=self.gamma*xhat+self.beta\n",
        "        return self.out\n",
        "    def parameters(self):\n",
        "        return [self.gamma,self.beta]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "K37DFM2SsdxN",
        "outputId": "62db69b4-3124-40a2-b824-2be99fb17802"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8064, 91])\n",
            "tensor(4.7231, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "from torch.nn import functional as F\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self,vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table=nn.Embedding(vocab_size,n_embd)\n",
        "        self.position_embedding_table=nn.Embedding(block_size,n_embd)\n",
        "        self.blocks=nn.Sequential(*[Block(n_embd,n_head=n_head) for _ in range(n_layer)])\n",
        "        self.lm_head=nn.Linear(n_embd,vocab_size)\n",
        "        self.ln_f=nn.LayerNorm(n_embd)\n",
        "\n",
        "\n",
        "    def forward(self,idx,targets=None):\n",
        "        B,T=idx.shape\n",
        "        tok_emb=self.token_embedding_table(idx)#B,T,C\n",
        "        pos_emb=self.position_embedding_table(torch.arange(T,device=device))#T,C\n",
        "        x=tok_emb+pos_emb #(B,T,C)\n",
        "        x=self.blocks(x)\n",
        "        x=self.ln_f(x)\n",
        "        logits=self.lm_head(x)#(B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss=None\n",
        "            # Return logits only when targets is None\n",
        "            return logits\n",
        "        else:\n",
        "          B,T,C=logits.shape\n",
        "          logits=logits.view(B*T,C)\n",
        "          targets=targets.view(B*T)\n",
        "          loss=F.cross_entropy(logits,targets)\n",
        "          return logits,loss\n",
        "\n",
        "    def generate(self,idx,max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Get logits only as the forward method will return only logits when targets is None\n",
        "            idx_cond=idx[:, -block_size:]\n",
        "            logits = self(idx_cond)\n",
        "            logits=logits[:,-1,:]\n",
        "            probs=F.softmax(logits,dim=-1)\n",
        "            idx_next=torch.multinomial(probs,num_samples=1)\n",
        "            idx=torch.cat((idx,idx_next),dim=1)\n",
        "        return idx\n",
        "\n",
        "m=BigramLanguageModel(vocab_size)\n",
        "logits,loss=m(xb,yb)\n",
        "print(logits.shape)\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrL2lf-01b5-",
        "outputId": "8368910e-a31c-4fdb-82c2-96806bb14c95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AahxpLJgmâLTR;L:KâdcëHVÀPd:é',“ZjX!gy\n",
            "u!!U‘CctcKDÀbÇkS œbp;lQ)”ELYQêR…ëLOâ“)-tâëXX lQCML\t]qFyc*Ç‘â_\n"
          ]
        }
      ],
      "source": [
        "idx=torch.zeros((1,1),dtype=torch.long)\n",
        "print(decode(m.generate(idx,max_new_tokens=100)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "m9s473V23KWC"
      },
      "outputs": [],
      "source": [
        "optimizer=torch.optim.AdamW(m.parameters(),lr=3e-4 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YBWViuFn3eny",
        "outputId": "e85a3db5-e4d4-4f8a-805c-81d03408cf7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.0953006744384766\n"
          ]
        }
      ],
      "source": [
        "batch_size=32\n",
        "for steps in range(500):\n",
        "    xb,yb=get_batch(\"train\")\n",
        "    logits,loss=m(xb,yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4GpVQ8UwDZrz",
        "outputId": "c3cb2698-1818-4e33-8f79-c013a1fd154d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "CFom nikery??\n",
            "\n",
            "CRANRAS.\n",
            "Of how a the sich whas offore will this tom of tir’s’s\n",
            "IMLO.\n",
            "MFrant; now to the for flo whej to a sirrch\n",
            "Ton mangselin, wit pesempp fontrongh, stend tomu, we brame\n",
            "Tho shall thet mur tecil.\n",
            "\n",
            "FAST.\n",
            "Pul o thene thro“Grd will Swrienptmuce and of swed you, gars, un\n",
            "then hush yort werd thirty that she kisgri of Ot fatir.\n",
            "\n",
            "BALSSW.\n",
            "A gro fright comnonemh to uptronice.\n",
            "Nou the of Lemplice aight sup it cantuiguinit.\n",
            "\n",
            "LIOOè.\n",
            "\n",
            "Com, me thad ’t wes, that a my I risem slad pean, and ford frign ouke\n",
            "Wi nock is _caye.]\n",
            "\n",
            "MINIUR.\n",
            "In thee the not tadence of of areniaga_.]\n",
            "\n",
            "FORIV._\n",
            "\n",
            " Got do warte if there couge!\n",
            "\n",
            "RARD.\n",
            "\n",
            "Cow Vort hurd care thou to hou wist that wo fair murth to up)\n",
            "HALL\n",
            "NUS.\n",
            "\n",
            " sisat it.\n",
            "Awith sunts spernait, tan the ton bueeat hom b-midid\n",
            "I wis thathe wind sate thare of or am tore,\n",
            "Weich d the this thou wolspece to hou hims and cÉa atosel.\n",
            "\n",
            "Ep’s Main.\n",
            "\n",
            "ALABRICH.\n",
            "Slower. Balive.\n",
            "\n",
            "LARIO.\n",
            "Mive gre whiseal on efidestr, Lon impin to by thanith with mads tra\n",
            "If you deai\n"
          ]
        }
      ],
      "source": [
        "print(decode(m.generate(idx=torch.zeros((1,1),dtype=torch.long),max_new_tokens=1000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ytAjZA19YA6v"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    def __init__(self,head_size):\n",
        "        super().__init__()\n",
        "        self.key=nn.Linear(n_embd,head_size,bias=False)\n",
        "        self.query=nn.Linear(n_embd,head_size,bias=False)\n",
        "        self.value=nn.Linear(n_embd,head_size,bias=False)\n",
        "        self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size)))\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self,x):\n",
        "      B,T,C=x.shape\n",
        "      k=self.key(x) #(B,T,C)\n",
        "      q=self.query(x)#(B,T,C)\n",
        "      # attention scores\n",
        "      wei=q@k.transpose(-2,-1)*C**-0.5 #(B,T,C)@(B,C,T)->(B,T,T)\n",
        "      wei=wei.masked_fill(self.tril[:T,:T]==0,float('-inf'))\n",
        "      wei=F.softmax(wei,dim=-1)\n",
        "      wei=self.dropout(wei)\n",
        "      v=self.value(x)\n",
        "      out=wei@v\n",
        "      return out\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7rGXkbSHYA3K"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Cc6mjJFGYA00"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "i4r1luuYYAyK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aLCNMZJfYAvj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bppOTHIYYAtD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hrwRAIh2YAqq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LDP8eaZbYASP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "j7rW1kMxYAO2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}